---
name: exreg
---

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

[Empirical question:]{.note}

: Does the number of on-campus police officers affect campus crime rates? If so, by how much?

. . .

<br>

Always plot your data first

---

## 
<!-- GENERATE DATA -->

```{r}
#| echo: false

library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, wooldridge, DT)
# Data 
data <- get(data(campus)) %>% 
  mutate(crime = round(crime/enroll*1000, 2),
         police = round(police/enroll*1000, 2)) %>% 
  filter(police < 10) %>% # remove outlier
  select(crime, police)
data2 <- get(data(campus)) %>% 
  mutate(crime = round(crime/enroll*1000, 2),
         police = round(police/enroll*1000, 2)) %>% 
  select(crime, police)
```

<!-- SLIDE HERE ABOUT HOW TO PICK BETA0 AND BETA1; PICK A LINE THAT MINIMIZES RESIDUAL -->

<!-- CREATE AN INTERACTIVE PLOT WITH SIM'D DATA ON SCATTER; DRAW NAIVE LINE -->

```{r}
#| echo: false
#| fig.height: 5.75

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

The scatter plot suggest that a _weak_ positive relationship exists

- A sample correlation of 0.14 confirms this

<br>

. . . 

_But correlation does not imply causation_

. . .

<br>

Lets estimate a statistical model 

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

We express the relationship between a [explained variable]{.hp} and an [explanatory variable]{.hii} as linear:

$$
{\color{#81A1C1} \text{Crime}_i} = \beta_1 + \beta_2 {\color{#B48EAD} \text{Police}_i} + u_i.
$$

- $\beta_1$ is the _intercept_ or constant.

- $\beta_2$ is the _slope coefficient_.

- $u_i$ is an _error term_ or disturbance term.

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

The [intercept]{.hi-red} tells us the expected value of $\text{Crime}_i$ when $\text{Police}_i = 0$. 

$$
\text{Crime}_i = {\color{#BF616A} \beta_1} + \beta_2\text{Police}_i + u_i
$$

Usually not the focus of an analysis.

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

The [slope coefficient]{.hi-red} tells us the expected change in $\text{Crime}_i$ when $\text{Police}_i$ increases by one. 

$$
 \text{Crime}_i = \beta_1 + {\color{#BF616A} \beta_2} \text{Police}_i + u_i
$$

"A one-unit increase in $\text{Police}_i$ is associated with a $\color{#BF616A}{\beta_2}$-unit increase in $\text{Crime}_i$."

. . .

Interpretation of this parameter is [crucial]{.hi}

. . .

Under certain (strong) assumptions^[Assumptions regarding the error term], $\color{#BF616A}{\beta_2}$ is the _effect of_ $X_i$ _on_ $Y_i$.

- Otherwise, it's the _association of_ $X_i$ _with_ $Y_i$.


---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

The [error term]{.hi-red} reminds us that $\text{Police}_i$ does not perfectly explain $Y_i$. 

$$
 \text{Crime}_i = \beta_1 + \beta_2\text{Police}_i + {\color{#BF616A} u_i}
$$

Represents all other factors that explain $\text{Crime}_i$.

- Useful mnemonic: pretend that $u$ stands for *"unobserved"* or *"unexplained."*


---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

How might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?

$$
 \text{Crime}_i = \beta_1 + \beta_2\text{Police}_i + u_i.
$$

- $\beta_1$ is the crime rate for colleges without police.
- $\beta_2$ is the increase in the crime rate for an additional police officer per 1000 students.

---

## [Ex.]{.ex} [Effect of police on crime]{.hi}

How might we apply the simple linear regression model to our question?

$$
 \text{Crime}_i = \beta_1 + \beta_2\text{Police}_i + u_i
$$

$\beta_1$ and $\beta_2$ are the unobserved population parameters we want

. . .

<br>

[We estimate]{.hii}

- $\hat{\beta_1}$ and $\hat{\beta_2}$ generate predictions of $\text{Crime}_i$ called $\widehat{\text{Crime}_i}$. 

- We call the predictions of the dependent variable [fitted values.]{.hi}

. . .

- Together, these trace a line: $\widehat{\text{Crime}_i} = \hat{\beta_1} + \hat{\beta_2}\text{Police}_i$.

---

##

::: {.vertical-center}
So, the question becomes, _how do I pick $\hat{\beta_1}$ and $\hat{\beta_2}$_
:::

---

##

[Let's take some guesses:]{.fragment}

```{r}
#| echo: false
#| fig.height: 5.75

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    mytheme_s +
    coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

<br>

---

## {data-visibility="uncounted"}

Let's take some guesses: $\hat{\beta_1} = 60$ and $\hat{\beta_2}=-7$

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 60
b1 <- -7

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

[Does this line represent the data well?]{.fragment}

---

## {data-visibility="uncounted"}

Let's take some guesses: $\hat{\beta_1} = 60$ and $\hat{\beta_2}=-7$

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 30
b1 <- 0

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

What about this one?


---

## {data-visibility="uncounted"}

Let's take some guesses: $\hat{\beta_1} = 60$ and $\hat{\beta_2}=-7$

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 15.6
b1 <- 7.94

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

Or this one?

---

## Residuals

Using $\hat{\beta_1}$ and $\hat{\beta_2}$ to make $\hat{Y_i}$ generates misses.

We call these misses [residuals:]{.hi}

$$
{\color{#BF616A} \hat{u}_i} = {\color{#BF616A}Y_i - \hat{Y_i}}.
$$

[AKA]{.note} ${\color{#BF616A}e_i}$.

---

## {data-visibility="uncounted"}

$\hat{\beta_1} = 60$ and $\hat{\beta_2}=-7$

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 15.6
b1 <- 7.94

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    geom_segment(aes(x = police, xend = police, y = crime, yend = y_hat(police, b0, b1)), color = hired, 
               size = 0.5, alpha = 1) +
    coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students")
```

Does this line represent the data well?

---

## Residuals

_What is we picked an estimator that minimizes the residuals?_

_Why not minimize_ 

$$
\sum_{i=1}^n \hat{u}_i^2
$$

_so that the estimator makes fewer big misses?_

. . .

This estimator, the [residual sum of squares]{.note} ([RSS]{.hii}), is convenient because _squared numbers are never negative_ so we can minimize an absolute sum of the residuals

---

##

[RSS]{.hii} gives bigger penalties to bigger residuals.

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 15.6
b1 <- 7.94

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    geom_segment(aes(x = police, xend = police, y = crime, yend = y_hat(police, b0, b1), size =  (crime - y_hat(police, b0, b1))^2), color = hii, alpha = 0.5) +
    # coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    scale_size(range = c(0.001,2)) +
    # scale_color_continuous() +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students") +
    theme(
      legend.position = 'none'
    )
```

---

## {data-visibility="uncounted"}

[RSS]{.hii} gives bigger penalties to bigger residuals.

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 30
b1 <- 0

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    geom_segment(aes(x = police, xend = police, y = crime, yend = y_hat(police, b0, b1), size =  (crime - y_hat(police, b0, b1))^2), color = hii, alpha = 0.5) +
    # coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    scale_size(range = c(0.001,2)) +
    # scale_color_continuous() +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students") +
    theme(
      legend.position = 'none'
    )
```


---

## {data-visibility="uncounted"}

[RSS]{.hii} gives bigger penalties to bigger residuals.

```{r}
#| echo: false
#| fig.height: 5.75

y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 60
b1 <- -7

data %>% 
  ggplot(aes(x = police, y = crime)) +
    geom_point(color = hi) +
    geom_abline(intercept = b0, slope = b1, color = hp, size = 1.25) +
    geom_segment(aes(x = police, xend = police, y = crime, yend = y_hat(police, b0, b1), size =  (crime - y_hat(police, b0, b1))^2), color = hii, alpha = 0.5) +
    # coord_fixed() +
    coord_cartesian(xlim = c(0,7), ylim = c(0,75)) +
    scale_size(range = c(0.001,2)) +
    # scale_color_continuous() +
    mytheme_s +
    xlab("Police per 1000 students") + 
    ylab("Crimes per 1000 students") +
    theme(
      legend.position = 'none'
    )
```

---

## Minimizing RSS

We could test thousands of guesses of $\beta_1$ and $\beta_2$ and pick the pair the has the smallest RSS

. . .

<br>
<br>
<br>

Or... [We could just do a little math]{.fragment}

<!-- ---

## Running Regressions

We estimate the regression coefficients using an estimator called .hi[Ordinary Least Squares] (OLS).

- Picks estimates that make $\hat{Y_i}$ as close as possible to $Y_i$ given the information we have on $X$ and $Y$.

- The residual sum of squares (RSS), $\sum_{i=1}^n (Y_i - \hat{Y_i})^2$, gives us an idea of how accurate our model is. 

- .hi[OLS] minimizes this sum. 
 
- We will dive into the details next class. -->

