---
name: multi
---

---

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4)) +
  geom_point(color = hi, size = 3) +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size"
  ) +
  mytheme

```

---

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4, group = expgroup, color = expgroup)) +
  geom_point(size = 3) +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  # scale_x_log10() + scale_y_log10() +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
  ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.9),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  )

```

---

## Multiple linear regression

[Simple linear regression]{.note} features one [outcome variable]{.hi} and one [explanatory variable]{.hii}:

$$
\color{#434C5E}{Y_i} = \beta_0 + \beta_1 \color{"#81A1C1"}{X_i} + u_i
$$

[Multiple linear regression]{.note} features one [outcome variable]{.hi} and multiple [explanatory variables]{.hii}:

$$
\color{#434C5E}{Y_i} = \beta_0 + \beta_1 \color{"#81A1C1"}{X_{1i}} + \beta_2 \color{"#81A1C1"}{X_{2i}} + \cdots + \beta_{k} \color{"#81A1C1"}{X_{ki}} + u_i
$$

. . .

This serves more than one purpose. Multiple [explanatory variables]{.hii} improves predictions, avoids [OVB]{.hi-red}, and better explains variation in $Y$.

---

## Multiple linear regression [Ex.]{.ex}

Controlling for [school funding]{.hi}

$$
\text{Scores}_i = \beta_0 + \beta_1 \text{Class Size}_i + \text{Expenditure}_i+ u_i
$$

```{r}
#| echo: false
#| escape: false

m00 = fixest::feols(score4 ~ stratio, schools_dt)
m01 = fixest::feols(score4 ~ stratio + expreg, schools_dt)

tab <- data.frame(
  v1 = c("Intercept", "", "Class size", "", "Expenditure", ""),
  v2 = rbind(
    c(781.196, -3.768, ""),
    c("(16.46)", "(0.61)", "")
  ) %>% as.vector(),
  v3 = rbind(
    c(674.930, -0.960, "0.013"),
    c("(16.46)", "(0.64)", "(0.002)")
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2"),
  align = c("l", rep("c", 3)),
  caption = ""
) %>%
row_spec(1:6, color = hi) %>%
row_spec(seq(2,6,2), color = "#c2bebe") %>%
row_spec(1:6, extra_css = "line-height: 110%;") %>%
column_spec(1, color = "black", italic = T)
tab %>% column_spec(2, bold = T)

```

---

## Multiple linear regression [Ex.]{.ex} {data-visibility="uncounted"}

Controlling for [school funding]{.hi}

$$
\text{Scores}_i = \beta_0 + \beta_1 \text{Class Size}_i + \text{Expenditure}_i+ u_i
$$

```{r}
#| echo: false
#| escape: false

m00 = fixest::feols(score4 ~ stratio, schools_dt)
m01 = fixest::feols(score4 ~ stratio + expreg, schools_dt)

tab <- data.frame(
  v1 = c("Intercept", "", "Class size", "", "Expenditure", ""),
  v2 = rbind(
    c(781.196, -3.768, ""),
    c("(16.46)", "(0.61)", "")
  ) %>% as.vector(),
  v3 = rbind(
    c(674.930, -0.960, "0.013"),
    c("(16.46)", "(0.64)", "(0.002)")
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2"),
  align = c("l", rep("c", 3)),
  caption = ""
) %>%
row_spec(1:6, color = hi) %>%
row_spec(seq(2,6,2), color = "#c2bebe") %>%
row_spec(1:6, extra_css = "line-height: 110%;") %>%
column_spec(1, color = "black", italic = T)
tab %>% column_spec(3, bold = T)

```

---

## {data-visibility="uncounted"}

Class funding ([U]{.hi}) [confounds]{.note} our estimates of smaller class sizes ([X]{.hi}) on test scores ([Y]{.hi}). 
<br>
<br>


<!-- DAG SLIDE -->

```{r}
#| label: dag-ex-ovb-02-again
#| echo: FALSE
#| fig.height: 3
#| fig.width: 6
#| fig-align: center

# Plot the full DAG
ggplot(
  data = dag_dt,
  aes(x = x, y = y, xend = xend, yend = yend)
) +
geom_point(
  size = 20,
  fill = "white",
  color = hi,
  shape = 21,
  stroke = 0.6
) +
geom_curve(
  aes(x = xa, y = ya, xend = xb, yend = yb),
  curvature = 0,
  arrow = arrow(length = unit(0.07, "npc")),
  color = hi,
  size = 1.2,
  lineend = "round"
) +
geom_text(
  data = dag_dt[, .(name, x, y, xend = x, yend = y)] %>% unique(),
  aes(x = x, y = y, label = name),
  family = "Fira Sans Medium",
  size = 8,
  color = hi,
  fontface = "bold"
) +
theme_void() +
theme(
  legend.position = "none",
) +
coord_cartesian(
  xlim = c(dag_dt[, min(x)] * 0.95, dag_dt[, max(x)] * 1.05),
  ylim = c(dag_dt[, min(y)] * 0.8, dag_dt[, max(y)] * 1.1)
)
```

_Any unobserved variable_ that connects a [backdoor path]{.note} between class size ([X]{.hi}) and test scores ([Y]{.hi}) will [bias]{.hi} our [point estimate]{.note} of $\beta_1$.

---

## {data-visibility="uncounted"}

Class funding ([U]{.hi}) [confounds]{.note} our estimates of smaller class sizes ([X]{.hi}) on test scores ([Y]{.hi}). Including data on school funding ([U]{.hi}) in a multiple linear regression allows us to close this back door path.

```{r}
#| label: dag-ex-ovb-03
#| echo: FALSE
#| fig.height: 3
#| fig.width: 6
#| fig-align: center

# Add indicators for paths
dag_dt[, `:=`(
  path1 = (name == "X" & to == "Y") | (name == "Y"),
  path2 = (name == "X" & to == "U") | (name == "U" & to == "Y") | (name == "Y")
)]

# Add alpha level
dag_dt[, `:=`(
  alpha = ifelse((name == "X" & to == "U") | (name == "U" & to == "X"), 0.3, 1)
)]

# Continue with the rest of your code...


# Plot the full DAG
ggplot(
  data = dag_dt,
  aes(x = x, y = y, xend = xend, yend = yend)
) +
geom_point(
  size = 20,
  fill = "white",
  color = hi,
  shape = 21,
  stroke = 0.6
) +
geom_curve(
  aes(x = xa, y = ya, xend = xb, yend = yb, alpha = alpha),
  curvature = 0,
  arrow = arrow(length = unit(0.07, "npc")),
  color = hi,
  size = 1.2,
  lineend = "round"
) +
geom_text(
  data = dag_dt[, .(name, x, y, xend = x, yend = y)] %>% unique(),
  aes(x = x, y = y, label = name),
  family = "Fira Sans Medium",
  size = 8,
  color = hi,
  fontface = "bold"
) +
theme_void() +
theme(
  legend.position = "none",
) +
coord_cartesian(
  xlim = c(dag_dt[, min(x)] * 0.95, dag_dt[, max(x)] * 1.05),
  ylim = c(dag_dt[, min(y)] * 0.8, dag_dt[, max(y)] * 1.1)
)
```

. . .

With all backdoor paths closed, [point estimates]{.note} of $\beta_1$ will no longer be biased and will return the population parameter of interest. 

---

::: {.vertical-center}
_How does it work?_ [We can think of it almost like demeaning.]{.fragment}
:::

---

###### {data-visibility="uncounted"}

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4)) +
  geom_point(color = hi, size = 3) +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size"
  ) +
  mytheme

```

---

###### {data-visibility="uncounted"}

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4, group = expgroup, color = expgroup)) +
  geom_point(size = 3) +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  # scale_x_log10() + scale_y_log10() +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
  ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.9),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  )

```


---

###### {data-visibility="uncounted"}

![](./control.gif)
```{r}
#| include: false
#| echo: false
#| eval: false


# Create the group means for each W
schools_dt[, `:=`(mean_stratio = mean(stratio), mean_score4 = mean(score4)), by = expgroup]


# Create six different data tables for the animation steps
dt_list <- list(
  #Step 1: Raw data only
  copy(schools_dt)[, c("mean_stratio", "mean_score4") := NA],
  #Step 2: Add x-lines
  copy(schools_dt)[, mean_score4 := NA],
  #Step 3: X de-meaned 
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, mean_stratio = 0, mean_score4 = NA)],
  #Step 4: Remove X lines, add Y
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, mean_stratio = NA)],
  #Step 5: Y de-meaned
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, score4 = score4 - mean_score4, mean_stratio = NA, mean_score4 = 0)],
  #Step 6: Raw demeaned data only
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, score4 = score4 - mean_score4, mean_stratio = NA, mean_score4 = NA)]
)

# Add time column and rbind all data tables together
time_labels <- c("1. Before correction", "2. Figure out what differences in class size are explained by student expenditure",
                 "3. Remove differences in class size explained by student expenditure", "4. Figure out what differences in test scores are explained by student expenditure",
                 "5. Remove differences in test scores explained by student expenditure", "6. After correction")

for (i in seq_along(dt_list)) {
  dt_list[[i]][, time := time_labels[i]]
}

# Combine all data tables into one
schools_dt_full <- rbindlist(dt_list)

# Create the animation
p <- ggplot(schools_dt_full, aes(x = stratio, y = score4, , group = expgroup, color = expgroup)) +
  geom_point(size = 2) +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  geom_vline(aes(xintercept = mean_stratio, color = as.factor(expgroup))) +
  geom_hline(aes(yintercept = mean_score4, color = as.factor(expgroup))) +
  guides(color = guide_legend(title = "expgroup")) +
  labs(
    title = 'Relationship between test scores and class size, controlling for a student expenditure \n{closest_state}',
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
    ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.2),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  ) +
  transition_states(time, transition_length = 2, state_length = 1) +
  ease_aes('sine-in-out')



anim_save(
  animation = p,
  filename = "control.gif",
  path = "./slides/006-multiple/",  # change this to your desired directory
  width = 10.5,
  height = 7,
  units = "in",
  res = 150,
  nframes = 200
)
```

---

## OLS Estimation

[Residuals]{.note} are now defined as:

. . .

$$
\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_{1i} - \hat{\beta}_2 X_{2i} - \cdots - \hat{\beta}_{k} X_{ki}
$$

. . .

As with SLR, OLS minimizes the sum of squared residuals ([RSS]{.hi-orange}).

. . .

$$
\begin{align*}
  {\color{#D08770} RSS} &= \sum_{i = 1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_{1i} - \hat{\beta}_2 X_{2i} - \cdots - \hat{\beta}_{k} X_{ki})^2 \\
                        &= \color{#D08770}{\sum_{i=1}^n \hat{u}_i^2}
\end{align*}
$$

which is a familiar expression.

---

## OLS Estimation

To obtain [point estimates]{.note}: 

$$
\min_{\hat{\beta}_0,\, \hat{\beta}_1,\, \dots \, \hat{\beta}_k} \quad \color{#D08770}{\sum_{i=1}^n \hat{u}_i^2}
$$

- Take partial derivatives of RSS with respect to each $\hat{\beta}$
- Set each derivative equal to zero
- Solve the system of $k+1$ equations^[$k+1$ due to the intercept.].

. . .

The algebra is cumbersome. We let [R]{.mono} do the heavy lifting.

---

## Coefficient Interpretation

[Model]{.hi}

$$
\color{}{Y_i} = \beta_0 + \beta_1 \color{}{X_{1i}} + \beta_2 \color{}{X_{2i}} + \cdots + \beta_{k} \color{}{X_{ki}} + u_i
$$

[Interpretation]{.hi}

- The intercept $\hat{\beta}_0$ is the average value of $Y_i$ when all of the explanatory variables are equal to zero.
- Slope parameters $\hat{\beta}_1, \dots, \hat{\beta}_{k}$ give us the change in $Y_i$ from a one-unit change in $X_j$, holding the other $X$ variables constant. 

---

## Algebraic properties of OLS

The OLS first-order conditions yield the same properties as before.

<br>

[1.]{.note} Residuals sum to zero: $\sum_{i=1}^n \hat{u_i} = 0$.

[2.]{.note} The sample covariance $X_i$ and the $\hat{u_i}$ is zero.

[3.]{.note} The point $(\bar{X_1}, \bar{X_2}, \dots, \bar{X_k}, \bar{Y})$ is on the fitted regression "line."

---

## Goodness of fit

Fitted values are defined similarly:

$$
\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \hat{\beta}_2 X_{2i} + \cdots + \hat{\beta}_{k} X_{ki}
$$

The formula for $R^2$ is the same as before:

$$
R^2 =\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}
$$


---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false

# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hired, hii , hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0),
  y  = c( 0.0,   -2.5,   -1.8,    2.0),
  r  = c( 1.9,    1.5,    1.5,    1.3),
  l  = c( "Y", "X[1]", "X[2]", "X[3]"),
  xl = c( 0.0,   -0.5,    1.6,   -1.0),
  yl = c( 0.0,   -2.5,   -1.9,    2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```


---

## Goodness of fit

Suppose we have [two]{.hi} models:

<br>

[Model 1]{.hi} $Y_i = \beta_0 + \beta_1 X_{1i} + u_i$.

[Model 2:]{.hi} $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + v_i$

<br>

. . .

[T/F?]{.note} Model 2 will yield a lower $R^2$ than Model 1.

. . .

<br>
<br>

(_Hint: Think of $R^2$ as $R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$._)

---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false
# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5),
  y  = c( 0.0,   -2.5),
  r  = c( 1.9,    1.5),
  l  = c( "Y", "X[1]"),
  xl = c( 0.0,   -0.5),
  yl = c( 0.0,   -2.5)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 1", size = 9, color = hi, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false
#| 
# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hii, hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5, -1.0),
  y  = c( 0.0,   -2.5, 2.0),
  r  = c( 1.9,    1.5, 1.3),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -0.5, -1.0),
  yl = c( 0.0,   -2.5, 2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 2", color = hi, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---

##

::: {.vertical-center}
[Problem:]{.hi-red} As we add variables to our model, $R^2$ *mechanically* increases.

<br>

[Let me show you this [problem]{.hi-red} with a [simulation]{.hii}]{.fragment}

:::

---

##

::: {.vertical-center}
Simulate a dataset of 10,000 observations on $y$ and 1,000 random $x_k$ variables, where 

$$
y \perp x_k \quad \forall x_k \; \text{s.t.} \; k = 1, 2, \dots, 1000
$$

<br>

[We have 1,000 independent variables that are [independent]{.note} to the dependent variable.]{.fragment} [Each $x_k$ has no relationship to $y$ whatsoever.]{.fragment}

:::


---

[Problem:]{.hi-red} As we add variables to our model, $\color{#314f4f}{R^2}$ *mechanically* increases.

[Pseudo-code:]{.mono}

::: {.pseudocode}
Generate 10,000 obs. on $y$

Generate 10,000 obs. on variables $x_1$ through $x_{1000}$

<br>

Regressions:

- LM<sub>1</sub>: Regress $y$ of $x_1$; record $R^2$
- LM<sub>2</sub>: Regress $y$ of $x_1$ and $x_2$; record $R^2$
- ...
- LM<sub>1000</sub>: Regress $y$ on $x_1$, $x_2$, ..., $x_{1000}$; record $R^2$
:::

---

[Problem:]{.hi-red} As we add variables to our model, $\color{#314f4f}{R^2}$ *mechanically* increases.

[R]{.mono} code for the simulation:

```{r}
#| eval: false
#| echo: true

# Generate data --------------------------------------------------------------
  # Set random seed
  set.seed(1234)
  # Generate data
  y <- rnorm(1e4)
  x <- matrix(data = rnorm(1e7), nrow = 1e4)
  x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)

# Simulation -----------------------------------------------------------------
  # Loop across regressions
  r_df <- mclapply(X = 1:(1e3-1), mc.cores = detectCores() - 1, FUN = function(i) {
    # Add one additional regressor every iteration
    tmp_reg <- lm(y ~ x[,1:(i+1)]) %>% summary()
    # Export R2
    data.frame(
      k = i + 1,
      r2 = tmp_reg %$% r.squared,
      r2_adj = tmp_reg %$% adj.r.squared
    )
  }) %>% bind_rows()
```

---

######

[Problem:]{.hi-red} As we add variables to our model, $R^2$ *mechanically* increases.

<br>

```{r}
#| include: false
#| eval: true
#| cache: true

# Set random seed
set.seed(1234)
# Generate data
  # Generate number of rows
  n = 1e4
  # Generate number of regressors
  k = 1e3
  # Create data set
  y = rnorm(n)
  x = matrix(data = rnorm(1e7), nrow = n)
  x %<>% cbind(matrix(data = 1, nrow = n, ncol = 1), x)

# Run simulation
  # Fit k models, adding 1 regressor each iteration
  fit = parallel::mclapply(
    1:1000,
    function(i) {
      # Print progress
      print(i)
      # Fit model
      fit = coef(.lm.fit(x[, 1:(i + 1)], y))
      # Calculate residuals
      residuals = y - x[, 1:(i + 1)] %*% fit
      # Calculate total sum of squares
      tss = sum((y - mean(y))^2)
      # Calculate R-squared + adjusted R-squared
      rsq = 1 - sum(residuals^2) / tss
      adj = 1 - ((1 - rsq) * (n - 1)) / (n - i - 1)
      dt = data.table(
        k = i,
        rsq = rsq,
        adj_rsq = adj
      )
      return(dt)
    }, mc.cores = parallelly::availableCores()
  ) %>% rbindlist()

```

```{r}
#| echo: false
#| fig.height: 6.25
#| fig-align: center
#| eval: true

ggplot(data = fit, aes(x = k, y = rsq)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.75, color = hired) +
geom_line(aes(y = adj_rsq), size = 0.2, alpha = 0, color = hp) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
mytheme
```

---

###### {data-visibility="uncounted"}

[Problem:]{.hi-red} As we add variables to our model, $R^2$ *mechanically* increases.

[One solution:]{.hii} Penalize for the number of variables, _e.g._, adjusted $R^2$:

```{r}
#| echo: false
#| fig.height: 6.25
#| fig-align: center
#| eval: true
ggplot(data = fit, aes(x = k, y = rsq)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.15, color = hired) +
geom_line(aes(y = adj_rsq), size = 2, alpha = 0.85, color = hii) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
mytheme
```

---

## Goodness of fit

[Problem:]{.hi-red} As we add variables to our model, $R^2$ *mechanically* increases.

[One solution:]{.hii} Penalize for the number of variables, _e.g._, adjusted $R^2$:

$$
\bar{R}^2 = 1 - \dfrac{\sum_i \left( Y_i - \hat{Y}_i \right)^2/(n-k-1)}{\sum_i \left( Y_i - \bar{Y} \right)^2/(n-1)}
$$

*Note:* Adjusted $R^2$ need not be between 0 and 1.

---

## Goodness of fit

So how do you find $R^2$ and $\bar{R^2}$ in [R]{.mono .big}? [`broom::glance()`]{.fragment}

. . .

```{r}
#| echo: true

# Run model
model = lm(score4 ~ stratio + expreg, schools_dt)

# Print results as tibble
broom::tidy(model)
```

<br>

. . . 


```{r}
#| echo: true
# Print R2 info as tibble
broom::glance(model)
```

---

## Multiple regression

There are [tradeoffs]{.hi} to remember as we add/remove variables:

:::: {.columns}

::: {.column width="50%"}
[Fewer variables]{.hi-red}

- Explains less variation in $y$
- Provide simple interpretations and visualizations
- More worried about omitted-variable bias
:::

::: {.column width="50%"}
[More variables]{.hii}

- More likely to find _spurious_ relationships^[Spurious meaning _statistically significant_ by coincidence---not reflective of true, population-level relationship]
- More difficult interpretation
- The [variance]{.note} of our [point estimates]{.note} will be bigger
- We still might have omitted-variable bias
:::

::::


---

## Multiple regression {data-visibility="uncounted"}

There are [tradeoffs]{.hi} to remember as we add/remove variables:

:::: {.columns}

::: {.column width="50%"}
[Fewer variables]{.hi-red}

- Explains less variation in $y$
- Provide simple interpretations and visualizations
- More worried about omitted-variable bias
:::

::: {.column width="50%"}
[More variables]{.hii}

- More likely to find _spurious_ relationships^[Spurious meaning _statistically significant_ by coincidence---not reflective of true, population-level relationship]
- More difficult interpretation
- [The variance of our point estimates will be bigger]{.hi}
- We still might have omitted-variable bias
:::

::::