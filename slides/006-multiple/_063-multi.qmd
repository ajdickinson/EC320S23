---
name: multi
---

---

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4)) +
  geom_point(color = hi) +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size"
  ) +
  mytheme

```

---

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4, group = expgroup, color = expgroup)) +
  geom_point() +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  # scale_x_log10() + scale_y_log10() +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
  ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.9),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  )

```

---

## Multiple linear regression

[Simple linear regression]{.note} features one [outcome variable]{.hi} and one [explanatory variable]{.hii}:

$$
\color{#434C5E}{Y_i} = \beta_0 + \beta_1 \color{"#81A1C1"}{X_i} + u_i
$$

[Multiple linear regression]{.note} features one [outcome variable]{.hi} and multiple [explanatory variables]{.hii}:

$$
\color{#434C5E}{Y_i} = \beta_0 + \beta_1 \color{"#81A1C1"}{X_{1i}} + \beta_2 \color{"#81A1C1"}{X_{2i}} + \cdots + \beta_{k} \color{"#81A1C1"}{X_{ki}} + u_i
$$

. . .

This serves more than one purpose. Multiple [explanatory variables]{.hii} improves predictions, avoids [OVB]{.hi-red}, and better explains variation in $Y$.

---

## {data-visibility="uncounted"}

Class funding ([U]{.hi}) [confounds]{.note} our estimates of smaller class sizes ([X]{.hi}) on test scores ([Y]{.hi}). 
<br>
<br>


<!-- DAG SLIDE -->

```{r}
#| label: dag-ex-ovb-02-again
#| echo: FALSE
#| fig.height: 3
#| fig.width: 6
#| fig-align: center

# Plot the full DAG
ggplot(
  data = dag_dt,
  aes(x = x, y = y, xend = xend, yend = yend)
) +
geom_point(
  size = 20,
  fill = "white",
  color = hi,
  shape = 21,
  stroke = 0.6
) +
geom_curve(
  aes(x = xa, y = ya, xend = xb, yend = yb),
  curvature = 0,
  arrow = arrow(length = unit(0.07, "npc")),
  color = hi,
  size = 1.2,
  lineend = "round"
) +
geom_text(
  data = dag_dt[, .(name, x, y, xend = x, yend = y)] %>% unique(),
  aes(x = x, y = y, label = name),
  family = "Fira Sans Medium",
  size = 8,
  color = hi,
  fontface = "bold"
) +
theme_void() +
theme(
  legend.position = "none",
) +
coord_cartesian(
  xlim = c(dag_dt[, min(x)] * 0.95, dag_dt[, max(x)] * 1.05),
  ylim = c(dag_dt[, min(y)] * 0.8, dag_dt[, max(y)] * 1.1)
)
```

_Any unobserved variable_ that connects a [backdoor path]{.note} between class size ([X]{.hi}) and test scores ([Y]{.hi}) will [bias]{.hi} our [point estimate]{.note} of $\beta_1$.

---

## {data-visibility="uncounted"}

Class funding ([U]{.hi}) [confounds]{.note} our estimates of smaller class sizes ([X]{.hi}) on test scores ([Y]{.hi}). Including data on school funding ([U]{.hi}) in a multiple linear regression allows us to close this back door path.

```{r}
#| label: dag-ex-ovb-03
#| echo: FALSE
#| fig.height: 3
#| fig.width: 6
#| fig-align: center

# Add indicators for paths
dag_dt[, `:=`(
  path1 = (name == "X" & to == "Y") | (name == "Y"),
  path2 = (name == "X" & to == "U") | (name == "U" & to == "Y") | (name == "Y")
)]

# Add alpha level
dag_dt[, `:=`(
  alpha = ifelse((name == "X" & to == "U") | (name == "U" & to == "X"), 0.3, 1)
)]

# Continue with the rest of your code...


# Plot the full DAG
ggplot(
  data = dag_dt,
  aes(x = x, y = y, xend = xend, yend = yend)
) +
geom_point(
  size = 20,
  fill = "white",
  color = hi,
  shape = 21,
  stroke = 0.6
) +
geom_curve(
  aes(x = xa, y = ya, xend = xb, yend = yb, alpha = alpha),
  curvature = 0,
  arrow = arrow(length = unit(0.07, "npc")),
  color = hi,
  size = 1.2,
  lineend = "round"
) +
geom_text(
  data = dag_dt[, .(name, x, y, xend = x, yend = y)] %>% unique(),
  aes(x = x, y = y, label = name),
  family = "Fira Sans Medium",
  size = 8,
  color = hi,
  fontface = "bold"
) +
theme_void() +
theme(
  legend.position = "none",
) +
coord_cartesian(
  xlim = c(dag_dt[, min(x)] * 0.95, dag_dt[, max(x)] * 1.05),
  ylim = c(dag_dt[, min(y)] * 0.8, dag_dt[, max(y)] * 1.1)
)
```

. . .

With all backdoor paths closed, [point estimates]{.note} of $\beta_1$ will no longer be biased and will return the population parameter of interest. 

---

::: {.vertical-center}
_How does it work?_ [We can think of it almost like demeaning.]{.fragment}
:::

---

###### {data-visibility="uncounted"}

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4)) +
  geom_point(color = hi) +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size"
  ) +
  mytheme

```

---

###### {data-visibility="uncounted"}

```{r}
#| echo: false
#| fig.align: center
#| fig.height: 6.75

ggplot(data = schools_dt, aes(x = stratio, y = score4, group = expgroup, color = expgroup)) +
  geom_point() +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  # scale_x_log10() + scale_y_log10() +
  labs(
    title = "Relationship between test scores and class size",
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
  ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.9),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  )

```


---

###### {data-visibility="uncounted"}

![](./control.gif)
```{r}
#| include: false
#| echo: false
#| eval: false


# Create the group means for each W
schools_dt[, `:=`(mean_stratio = mean(stratio), mean_score4 = mean(score4)), by = expgroup]

# Create six different data tables for the animation steps
dt_list <- list(
  #Step 1: Raw data only
  copy(schools_dt)[, c("mean_stratio", "mean_score4") := NA],
  #Step 2: Add x-lines
  copy(schools_dt)[, mean_score4 := NA],
  #Step 3: X de-meaned 
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, mean_stratio = 0, mean_score4 = NA)],
  #Step 4: Remove X lines, add Y
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, mean_stratio = NA)],
  #Step 5: Y de-meaned
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, score4 = score4 - mean_score4, mean_stratio = NA, mean_score4 = 0)],
  #Step 6: Raw demeaned data only
  copy(schools_dt)[, `:=`(stratio = stratio - mean_stratio, score4 = score4 - mean_score4, mean_stratio = NA, mean_score4 = NA)]
)

# Add time column and rbind all data tables together
time_labels <- c("1. Before correction", "2. Figure out what differences in class size are explained by student expenditure",
                 "3. Remove differences in class size explained by student expenditure", "4. Figure out what differences in test scores are explained by student expenditure",
                 "5. Remove differences in test scores explained by student expenditure", "6. After correction")

for (i in seq_along(dt_list)) {
  dt_list[[i]][, time := time_labels[i]]
}

# Combine all data tables into one
schools_dt_full <- rbindlist(dt_list)

# Create the animation
p <- ggplot(schools_dt_full, aes(x = stratio, y = score4, , group = expgroup, color = expgroup)) +
  geom_point() +
  scale_color_manual(values = c(hi, hii), labels = c("Per student expenditure < $6,000", "Per student expenditure > $6,000")) +
  geom_vline(aes(xintercept = mean_stratio, color = as.factor(expgroup))) +
  geom_hline(aes(yintercept = mean_score4, color = as.factor(expgroup))) +
  guides(color = guide_legend(title = "expgroup")) +
  labs(
    title = 'Relationship between test scores and class size, controlling for a student expenditure \n{closest_state}',
    subtitle = "Test scores are aggregated math, reading, and science scores among 4th graders",
    y = "Test scores",
    x = "Class size",
    color = NULL
    ) +
  mytheme +
  theme(
    legend.position = c(0.8, 0.2),
    legend.text = element_text(size = 12),
    legend.background = element_rect(color = hi, size = 0.5)
  ) +
  transition_states(time, transition_length = 2, state_length = 1) +
  ease_aes('sine-in-out')



anim_save(
  animation = p,
  filename = "control.gif",
  path = "./slides/006-multiple/",  # change this to your desired directory
  width = 10.5,
  height = 7,
  units = "in",
  res = 150,
  nframes = 200
)
```

---

## OLS Estimation

As was the case with simple linear regressions, OLS minimizes the sum of squared residuals (RSS).

However, residuals are now defined as

$$
\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_{1i} - \hat{\beta}_2 X_{2i} - \cdots - \hat{\beta}_{k} X_{ki}
$$

. . .

To obtain estimates, take partial derivatives of RSS with respect to each $\hat{\beta}$, set each derivative equal to zero, and solve the system of $k+1$ equations.

- Without matrices, the algebra is difficult. For the remainder of this course, we will let [R]{.mono} do the work for us.

---

## Coefficient Interpretation

[Model]{.hi}

$$
\color{}{Y_i} = \beta_0 + \beta_1 \color{}{X_{1i}} + \beta_2 \color{}{X_{2i}} + \cdots + \beta_{k} \color{}{X_{ki}} + u_i
$$

[Interpretation]{.hi}

- The intercept $\hat{\beta}_0$ is the average value of $Y_i$ when all of the explanatory variables are equal to zero.
- Slope parameters $\hat{\beta}_1, \dots, \hat{\beta}_{k}$ give us the change in $Y_i$ from a one-unit change in $X_j$, holding the other $X$ variables constant. 

---

## Algebraic properties of OLS

The OLS first-order conditions yield the same properties as before.

1. Residuals sum to zero: $\sum_{i=1}^n \hat{u_i} = 0$.

2. The sample covariance between the independent variables and the residuals is zero.

3. The point $(\bar{X_1}, \bar{X_2}, \dots, \bar{X_k}, \bar{Y})$ is always on the fitted regression "line."

---

## Goodness of fit

Fitted values are defined similarly:

$$
\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \hat{\beta}_2 X_{2i} + \cdots + \hat{\beta}_{k} X_{ki}
$$

The formula for $R^2$ is the same as before:

$$
R^2 =\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}
$$

---

## Goodness of fit

[Model 1]{.hi} $Y_i = \beta_0 + \beta_1 X_{1i} + u_i$.

[Model 2:]{.hi} $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + v_i$

. . .

[T/F?]{.note}

Model 2 will yield a lower $R^2$ than Model 1.

- Hint: Think of $R^2$ as $R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$.

---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false

# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hired, hii , hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0),
  y  = c( 0.0,   -2.5,   -1.8,    2.0),
  r  = c( 1.9,    1.5,    1.5,    1.3),
  l  = c( "Y", "X[1]", "X[2]", "X[3]"),
  xl = c( 0.0,   -0.5,    1.6,   -1.0),
  yl = c( 0.0,   -2.5,   -1.9,    2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false
# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hii)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5),
  y  = c( 0.0,   -2.5),
  r  = c( 1.9,    1.5),
  l  = c( "Y", "X[1]"),
  xl = c( 0.0,   -0.5),
  yl = c( 0.0,   -2.5)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 1", size = 9, color = hi, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---

## Goodness of fit

We can describe the variation explain in $Y$ with venn diagrams

```{r}
#| echo: false
#| 
# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hired, hii)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5, -1.0),
  y  = c( 0.0,   -2.5, 2.0),
  r  = c( 1.9,    1.5, 1.3),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -0.5, -1.0),
  yl = c( 0.0,   -2.5, 2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 2", color = hi, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---

## Goodness of fit

[Problem:]{.hi} As we add variables to our model, $R^2$ *mechanically* increases.

. . .

To see this problem, we can simulate a dataset of 10,000 observations on $y$ and 1,000 random $x_k$ variables. **No relations between $y$ and the $x_k$!**

Pseudo-code outline of the simulation:
. . .

[

- Generate 10,000 observations on $y$
- Generate 10,000 observations on variables $x_1$ through $x_{1000}$
- Regressions
  - LM<sub>1</sub>: Regress $y$ on $x_1$; record R<sup>2</sup>
  - LM<sub>2</sub>: Regress $y$ on $x_1$ and $x_2$; record R<sup>2</sup>
  - ...
  - LM<sub>1000</sub>: Regress $y$ on $x_1$, $x_2$, ..., $x_{1000}$; record R<sup>2</sup>
]{.mono .small}

---

## Goodness of fit

[Problem:]{.hi} As we add variables to our model, $R^2$ *mechanically* increases.

[R]{.mono} code for the simulation:

```{r}
#| eval: false
#| cache: true

set.seed(1234)

y <- rnorm(1e4)
x <- matrix(data = rnorm(1e7), nrow = 1e4)
x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)
r_fun <- function(i) {
 tmp_reg <- lm(y ~ x[,1:(i + 1)]) %>% summary()
                    data.frame(
                    k = i + 1,
                    r2 = tmp_reg$r.squared,
                    r2_adj = tmp_reg$adj.r.squared)
}
r_df <- parallel::mclapply(1:(1e3-1), r_fun, mc.cores = 6) %>% bind_rows()
```

---

## Goodness of fit

**Problem:** As we add variables to our model, $\color{#314f4f}{R^2}$ *mechanically* increases.

```{r}
#| include: false
#| cache: true

set.seed(1234)
plan()
y <- rnorm(1e4)
x <- matrix(data = rnorm(1e7), nrow = 1e4)
x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)

r_fun <- function(i) {
  tmp_reg <- lm(y ~ x[,1:(i + 1)]) %>% summary()
                     data.frame(
                     k = i + 1,
                     r2 = tmp_reg$r.squared,
                     r2_adj = tmp_reg$adj.r.squared)
}

r_fun <- function(i) {
  fit = coef(.lm.fit(x[, 1:(i + 1)], y))
  }

rsq_fun = function(i) {
  # calculate residuals
  residuals = y - x[, 1:(i + 1)] %*% fit[[i]]

  # calculate total sum of squares
  tss = sum((y - mean(y))^2)

  # calculate R-squared
  rsq = 1 - sum(residuals^2) / tss
  adj_r_squared = 1 - ((1 - r_squared) * (n - 1)) / (n - p - 1)

  out = c(rsq, adj_rsq)

  return(out)
}
  

fit = parallel::mclapply(1:(1e5), r_fun, mc.cores = 6)
rsq_mt = lapply(1:(1e5), rsq_fun) %>% unlist() %>% as.data.table()
```

```{r}
#| echo: false
#| fig.height: 6.25

ggplot(data = r_df, aes(x = k, y = r2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.75, color = hi) +
geom_line(aes(y = r2_adj), size = 0.2, alpha = 0, color = hp) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
mytheme
```

---

## Goodness of fit

[One solution:]{.hi} [Adjusted]{.hp} $\color{#B48EAD}{R^2}$

```{r}
#| echo: false
#| fig.height: 6.25
ggplot(data = r_df, aes(x = k, y = r2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.15, color = hi) +
geom_line(aes(y = r2_adj), size = 2, alpha = 0.85, color = hp) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
mytheme
```

---

## Goodness of fit

[Problem:]{.hi} As we add variables to our model, $R^2$ *mechanically* increases.

[One solution:]{.hi} Penalize for the number of variables, _e.g._, adjusted $R^2$:

$$
\bar{R}^2 = 1 - \dfrac{\sum_i \left( Y_i - \hat{Y}_i \right)^2/(n-k-1)}{\sum_i \left( Y_i - \bar{Y} \right)^2/(n-1)}
$$

*Note:* Adjusted $R^2$ need not be between 0 and 1.

