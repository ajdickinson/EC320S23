---
name: colinear
---

---

## OLS variances

Multiple regression model:

$$
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_{k} X_{ki} + u_i
$$

It can be shown that the estimator $\hat{\beta}_j$ on independent variable $X_j$ is:

$$
\mathop{\text{Var}} \left( \hat{\beta_j} \right) = \dfrac{\sigma^2}{\left( 1 - R^2_j \right)\sum_{i=1}^n \left( X_{ji} - \bar{X}_j \right)^2},
$$

where $R^2_j$ is the $R^2$ from a regression of $X_j$ on the other independent variables and the intercept

---

## OLS variances

$$
\mathop{\text{Var}} \left( \hat{\beta_j} \right) = \dfrac{{\color{#81A1C1}\sigma^2}}{\left( 1 - {\color{#81A1C1}R_j^2} \right){\color{#BF616A}\sum_{i=1}^n \left( X_{ji} - \bar{X}_j \right)^2}},
$$

[Moving parts:]{.hi}

::: {.small}
[[1.]{.note} [Error variance:]{.hi} As ${\color{#81A1C1}\sigma^2}$ increases, $\Var(\hat{\beta}_j)$ increases]{.fragment}

[[2.]{.note} [Total variation in $X_j$:]{.hi} As ${\color{#BF616A}\sum_{i=1}^n \left( X_{ji} - \bar{X}_j \right)^2}$ increases, $\Var(\hat{\beta}_j)$ decreases]{.fragment}

[[3.]{.note} [Relationship across $X_i$:]{.hi} As ${\color{#81A1C1}R_j^2}$ increases, $\Var(\hat{\beta}_j)$ increases]{.fragment}
:::

<br>

. . .

[3.]{.note} is better known as [Multicollinearity]{.note}

---

## Multicollinearity

> Case in which two or more independent variables in a regression model are highly correlated.

. . .

<br>

One independent variable can phiredict most of the variation in another independent variable.

<br>

. . .

[Multicollinearity]{.note} leads to [imprecise]{.hi} estimates. [Becomes difficult to distinguish between individual effects from of independent variables.]{.fragment}

---

## OLS Assumptions

Classical assumptions for OLS change slightly for multiple OLS

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [[Sample Variation:]{.hi} No $X$ variable is a perfect linear combination of the others]{.hi}

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 

[A4.]{.note} [Homoskedasticity:]{.hi} The error term has the same variance for each value of the independent variable 

[A5.]{.note} [Non-autocorrelation:]{.hi} The values of error terms have independent distributions 

---

## Perfect Collinearity

> Case in which two or more independent variables in a regression model are perfectly correlated.


[Ex.]{.ex} 2016 Election

OLS simultaneously cannot estimate parameters for [white]{.mono} and [nonwhite]{.mono}.

. . .

```{r}
#| echo: true
lm(trump_margin ~ white + nonwhite, data = election) %>% tidy()
```

[R]{.mono} drops perfectly collinear variables for you.

---

## Multicollinearity [Ex.]{.ex}

Suppose that we want to understand the relationship between crime rates and poverty rates in US cities. We could estimate the model

$$
\text{Crime}_i = \beta_0 + \beta_1 \text{Poverty}_i + \beta_2 \text{Income}_i + u_i
$$

. . .

Before obtaining standard errors, we need:

$$
\mathop{\text{Var}} \left( \hat{\beta}_1 \right) = \dfrac{\sigma^2}{\left( 1 - R^2_1 \right)\sum_{i=1}^n \left( \text{Poverty}_{i} - \overline{\text{Poverty}} \right)^2}
$$

. . .

$R^2_1$ is the $R^2$ from a regression of poverty on median income:

$$
\text{Poverty}_i = \gamma_0 + \gamma_1 \text{Income}_i + v_i
$$

---

## Multicollinearity

[Scenario 1:]{.hi} $\text{Income}_i$ explains most variation in $\text{Poverty}_i$, then $R^2_1 \rightarrow 1$

- Violates the [_no perfect collinearity_]{.note} assumption

. . .

[Scenario 2:]{.hi} If $\text{Income}_i$ explains no variation in $\text{Poverty}_i$, then $R^2_1 = 0$

. . .

[Q.]{.note} _In which scenario is the variance of the poverty coefficient smaller?_

$$
\mathop{\text{Var}} \left( \hat{\beta}_1 \right) = \dfrac{\sigma^2}{\left( 1 - R^2_1 \right)\sum_{i=1}^n \left( \text{Poverty}_{i} - \overline{\text{Poverty}} \right)^2}
$$

. . .

[A.]{.note} [Scenario 2.]{.hi}

---

## Multicollinearity

```{r}
#| echo: false


# Colors (order: x1, x2, y)
venn_colors <- c(hii, hired, hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.1,    0.1),
  y  = c( 0.0,   -2.5,   -2.5),
  r  = c( 1.9,    1.3,    1.3),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -1.75,    1.75),
  yl = c( 0.0,   -2.5,   -2.5)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
  annotate(
  x = 0, y = 2.25,
  geom = "text", label = "Scenario 1", color = hi, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3) +
coord_equal()
```

---

## Multicollinearity

```{R, echo = F, dev = "svg"}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(hii, hired, hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    2),
  y  = c( 0.0,   -2.5,   0.5),
  r  = c( 1.9,    1.3,    1.3),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -0.5,    2.5),
  yl = c( 0.0,   -2.5,   0.5)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
  annotate(
  x = 0, y = 2.25,
  geom = "text", label = "Scenario 2", color = hi, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3.5) +
ylim(-4, 3) +
coord_equal()
```

---

## Multicollinearity

As the relationships between the variables increase, $R^2_j$ increases.

For high $R^2_j$, $\mathop{\text{Var}} \left( \hat{\beta_j} \right)$ is large:

$$
\mathop{\text{Var}} \left( \hat{\beta_j} \right) = \dfrac{\sigma^2}{\left( 1 - R^2_j \right)\sum_{i=1}^n \left( X_{ji} - \bar{X}_j \right)^2}
$$

. . .

- Some view multicollinearity as a "problem" to be solved.
- Either increase power ($n$) or drop correlated variables
- [Warning:]{.note} Dropping variables can generate omitted variable bias.

---

## Irrelevant Variables

Suppose that the true relationship between birth weight and _in utero_ exposure to toxic air pollution is 

$$
(\text{Birth Weight})_i = \beta_0 + \beta_1 \text{Pollution}_i + u_i
$$

. . .

Suppose that an "analyst" estimates

$$
(\text{Birth Weight})_i = \tilde{\beta_0} + \tilde{\beta_1} \text{Pollution}_i + \tilde{\beta_2}\text{NBA}_i + u_i
$$

. . .

One can show that $\mathop{\mathbb{E}} \left( \hat{\tilde{\beta_1}} \right) = \beta_1$ (*i.e.*, $\hat{\tilde{\beta_1}}$ is unbiased).

However, the variances of $\hat{\tilde{\beta_1}}$ and $\hat{\beta_1}$ differ.

---

## Irrelevant Variables

```{r}
#| echo: false

# Colors (order: x1, x2, x3, y)
venn_colors <- c(hp, hired, hi)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    2.3),
  y  = c( 0.0,   -2.5,   -3),
  r  = c( 1.9,    1.5,    1.5),
  l  = c( "Weight", "Pollution", "NBA"),
  xl = c( 0.0,   -0.5,    2.3),
  yl = c( 0.0,   -2.5,   -3)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
xlim(-5.5, 4.5) +
ylim(-4.5, 3) +
coord_equal()
```

---

## Irrelevant Variables

The variance of $\hat{\beta}_1$ from estimating the "true model" is

$$
\mathop{\text{Var}} \left( \hat{\beta_1} \right) = \dfrac{\sigma^2}{\sum_{i=1}^n \left( \text{Pollution}_{i} - \overline{\text{Pollution}} \right)^2}
$$

The variance of $\hat{\tilde\beta}_1$ from estimating the model with the irrelevant variable is

$$
\mathop{\text{Var}} \left( \hat{\tilde{\beta_1}} \right) = \dfrac{\sigma^2}{\left( 1 - R^2_1 \right)\sum_{i=1}^n \left( \text{Pollution}_{i} - \overline{\text{Pollution}} \right)^2}
$$

---

## Irrelevant Variables

Notice that $\mathop{\text{Var}} \left( \hat{\beta_1} \right) \leq \mathop{\text{Var}} \left( \hat{\tilde{\beta_1}} \right)$ since,

$$
\sum_{i=1}^n \left( \text{Poll.}_{i} - \overline{\text{Poll.}} \right)^2
\geq
\left( 1 - R^2_1 \right)\sum_{i=1}^n \left( \text{Poll.}_{i} - \overline{\text{Poll.}} \right)^2
$$

. . .

<br>

A tradeoff exists when including more control variables. [Make sure you have good reason for your controls because including irrelevant control variables [increase]{.hii} variances]{.fragment}

---

## Estimating Error Variance

We cannot observe $\sigma^2$, so we must estimate it using the residuals from an estimated regression:

$$
s_u^2 = \dfrac{\sum_{i=1}^n \hat{u}_i^2}{n - k - 1}
$$

- $k+1$ is the number of parameters (one "slope" for each $X$ variable and an intercept).
- $n - k - 1$ [=]{.mono} degrees of freedom.
- Using the first 5 OLS assumptions, one can prove that $s_u^2$ is an unbiased estimator of $\sigma^2$.

---

## Standard Errors

The formula for the standard error is the square root of $\mathop{\text{Var}} \left( \hat{\beta_j} \right)$:

$$
\mathop{\text{SE}}(\hat{\beta_j}) = \sqrt{ \frac{s^2_u}{(  1 - R^2_j ) \sum_{i=1}^n ( X_{ji} - \bar{X}_j )^2} }
$$

---

## Multicollinearity [Ex.]{.ex}

Suppose I run the following model:

$$
\text{Scores}_i = \beta_0 + \beta_1 \text{Class Size}_i + \text{Lunch}_i+ u_i
$$

with the following results:


```{r}
#| echo: false
#| escape: false

m00 = lm(score4 ~ stratio + lunch, schools_dt) %>% tidy() 
m01 = lm(score4 ~ stratio + lunch + income, schools_dt) %>% tidy()

coef_fun = function(x) {
  x %>% as.numeric() %>% round(2)
}

se_fun = function(x) {
  x %>% as.numeric() %>% round(2) %>% paste0("(",.,")")
}

tab <- data.frame(
  v1 = c("Intercept", "", "Class size", "", "Lunch", "", "Income", ""),
  v2 = rbind(
    c(m00[1,2] %>% coef_fun(), m00[2,2] %>% coef_fun(), m00[3,2] %>% coef_fun(), ""),
    c(m00[1,3] %>% se_fun(), m00[2,3] %>% se_fun(), m00[3,3] %>% se_fun(), "")
  ) %>% as.vector(),
  v3 = rbind(
    c(m01[1,2] %>% coef_fun(), m01[2,2] %>% coef_fun(), m01[3,2] %>% coef_fun(), m01[4,2] %>% coef_fun()),
    c(m01[1,3] %>% se_fun(), m01[2,3] %>% se_fun(), m01[3,3] %>% se_fun(),  m01[4,3] %>% se_fun())
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2"),
  align = c("l", rep("c", 3)),
  caption = ""
) %>%
row_spec(1:6, color = hi) %>%
row_spec(seq(2,6,2), color = "#c2bebe") %>%
row_spec(1:6, extra_css = "line-height: 110%;") %>%
kable_styling(font_size = 24) %>%  
column_spec(1, color = "black", italic = T)
tab %>% column_spec(2, bold = T)

```

---

## Multicollinearity [Ex.]{.ex} {data-visibility="uncounted"}

Suppose I run the following model:

$$
\text{Scores}_i = \beta_0 + \beta_1 \text{Class Size}_i + \text{Lunch}_i+ u_i
$$

with the following results:


```{r}
#| echo: false
#| escape: false

m00 = fixest::feols(score4 ~ stratio + lunch, schools_dt) %>% tidy()
m01 = fixest::feols(score4 ~ stratio + lunch + income, schools_dt) %>% tidy()

tab <- data.frame(
  v1 = c("Intercept", "", "Class size", "", "Lunch", "", "Income", ""),
  v2 = rbind(
    c(m00[1,2] %>% coef_fun(), m00[2,2] %>% coef_fun(), m00[3,2] %>% coef_fun(), ""),
    c(m00[1,3] %>% se_fun(), m00[2,3] %>% se_fun(), m00[3,3] %>% se_fun(), "")
  ) %>% as.vector(),
  v3 = rbind(
    c(m01[1,2] %>% coef_fun(), m01[2,2] %>% coef_fun(), m01[3,2] %>% coef_fun(), m01[4,2] %>% coef_fun()),
    c(m01[1,3] %>% se_fun(), m01[2,3] %>% se_fun(), m01[3,3] %>% se_fun(),  m01[4,3] %>% se_fun())
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2"),
  align = c("l", rep("c", 3)),
  caption = ""
) %>%
row_spec(1:6, color = hi) %>%
row_spec(seq(2,6,2), color = "#c2bebe") %>%
row_spec(1:6, extra_css = "line-height: 110%;") %>%
kable_styling(font_size = 24) %>%  
column_spec(1, color = "black", italic = T)
tab %>% column_spec(3, bold = T)

```