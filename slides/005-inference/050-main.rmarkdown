---
title: "Inference"
subtitle: "EC 320, Set 06"
author: "Andrew Dickinson"
date: last-modified
date-format: "Spring YYYY"
format: 
  revealjs:
    theme: [default, ../styles.scss]
    monobackgroundcolor: #F5F5F5
    slide-number: true
    footer: "EC320, Set 06 | Inference"
    preview-links: auto
    code-fold: FALSE
    code-copy: TRUE
    highlight-style: a11y-light
    cache: FALSE
    html-math-method: mathjax 
title-slide-attributes: 
  data-background-position: left
---


::: {.content-hidden}
$$
% Load required package
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{enumitem}

\newcommand{\ci}{\perp\mkern-10mu\perp}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}


$$
:::

## Housekeeping

[PS04:]{.hi}

- Will be assigned next week (_enjoy the warm weather_)
- Due next _next_ Tuesday

. . .

[LA05:]{.hi}

- Due Friday at 5:00p

. . .

[Reading:]{.hi} _(up to this point)_

ItE: R, 1, [2]{.hi}
MM: 1, [2]{.hi}

---

## MT scores

![](mt-scores.png)


# _Prologue_ {.inverse .note}


```{r}
pacman::p_load(hrbrthemes, fastverse, tidyverse,
               magrittr, wooldridge, here, kableExtra,
               ggdag, nord, latex2exp, dagitty, viridis,
               plotly, ggforce, latex2exp, parallel, broom)


hi = nord_palettes$polarnight[3]
hii = nord_palettes$frost[3] 
hp = nord_palettes$aurora[5]
higreen = nord_palettes$aurora[4]
hiyellow = nord_palettes$aurora[3]
hiorange = nord_palettes$aurora[2]
hired = nord_palettes$aurora[1]
higrey = nord_palettes$snowstorm[1]

mytheme = theme_ipsum(base_family = "Fira Sans Book", base_size = 20) +
 theme(panel.grid.minor.x = element_blank(),
       axis.title.x = element_text(size = 18),
       axis.title.y = element_text(size = 18))

mytheme_s = mytheme + 
  theme(panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        axis.line = element_line(color = hi))

mytheme_void = theme_void(base_family = "Fira Sans Book", base_size = 20)


campus <- get(data(campus)) %>% 
  mutate(crime = round(crime/enroll*1000, 2),
         police = round(police/enroll*1000, 2)) %>% 
  filter(police < 10) %>% # remove outlier
  select(crime, police)

```
---
name: prologue
---

---

## OLS

Up to this point, we have been focusing on OLS considering:

- how we model regressions with this estimator
- how the estimator is derived and what properties it demonstrates
- how the classical assumptions make the estimator [BLUE]{.hii}

. . .

<br>

We have _mostly ignored_ drawing conclusions about the true population parameters from the estimates of the sample data. [AKA [_inference_.]{.note}]{.fragment}

---

## OLS

Thus far in this class we've fit an OLS model the following questions:

<!-- - Does living on campus at the UO increase student welfare?
- Do school-based de-worming interventions provide a cost-effective way to increase school attendance? -->
- _How much does an additional year of schooling increase earnings?_
- _Does the number of police officers affect campus crime rates?_
- _Are work training programs helpful at increasing earnings?_^[From the midterm.]

<br>

. . .

Though we've not discussed our confidence in our fitted relationship

. . .

Even if all [6]{.hi} assumptions hold, sample selection might generate the [incorrect conclusions]{.hi} in a completely unbiased, [coincidental]{.hi} fashion.


---

## Classical Assumptions

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 

. . .

[A4.]{.note} [Homoskedasticity:]{.hi} The error term has the same variance for each value of the independent variable 

[A5.]{.note} [Non-autocorrelation:]{.hi} The values of error terms have independent distributions 

. . .

_[A6.]{.note} [Normality:]{.hi} The population error term in normally distributed with mean zero and variance $\sigma^2$_

---

[_Previously_]{.note} we used the first [3]{.hi} assumptions to show that OLS is unbiased: 

$$
\mathop{\mathbb{E}}\left[ \hat{\beta} \right] = \beta
$$

<br>

We used the first [5]{.hi} assumptions to derive a formula for the [_variance_]{.note} of the OLS estimator: 

$$
\mathop{\text{Var}}(\hat{\beta}) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}
$$

. . .

By using the [_variance_]{.note} of the OLS estimator, we can infer confidence from the [sampling distribution]{.hi}

---

## Sampling distribution

> The probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting [_point estimates_]{.note} each time.

Provides information about their variability, accuracy, and precision across different samples.

. . .

<br>

[_Point estimates_]{.note}

> The fitted values of the OLS estimator (e.g., $\hat{\beta}_0, \hat{\beta}_1$)


---

## Sampling distribution properties

[1.]{.note} [Unbiasedness:]{.hi} If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., $E(\hat{\beta}_0) = \beta_0\) and \(E(\hat{\beta}_1) = \beta_1$)

. . .

[2.]{.note} [Variance:]{.hi} The variance of the OLS estimators describes their dispersion around the true population parameters.

. . .

[3.]{.note} [Normality:]{.hi} If the errors are normally distributed or the sample size is large enough, by the [CLT]{.hi}, the sampling distribution of the OLS estimators will be approximately normal.^[Useful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution.]

---

## Sampling distribution

The sampling distribution of $\hat{\beta}$ to conduct hypothesis tests.

Use all [6]{.hi} classical assumptions to show that OLS is [normally distributed]{.note}:

$$
\hat{\beta} \sim \mathop{N}\left( \beta, \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2} \right)
$$

<br>

. . .

To "prove" this, recall our simulation from last time

---

## 

```{r}
#| label: gen-dataset
#| include: false
#| cache: true

# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 4, X = 1:1e3, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

```{r}
#| label: sim-scatter
#| echo: false
#| fig-dpi: 300
#| cache: true
#| fig.height: 6
#| fig.align: center

# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = hi) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = hp, size = 1.5
) +
coord_cartesian(xlim = c(0, 10), ylim = c(0,10)) +
theme_void() +
theme(
  legend.position = "none"
)
```

---

Plotting the distributions of the [point estimates]{.note} in a histogram

<br>
<br>

:::: {.columns}

::: {.column width="50%"}
```{r}
#| label: simulation-hist1
#| echo: false
#| fig-dpi: 300
#| cache: true
#| fig.height: 5.75

# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_histogram(data = line_df, aes(intercept), fill = hp, alpha = 0.75, bins = 40) +
  geom_vline(xintercept = lm0$coefficients[1], size = 2, color = hi) +
  scale_x_continuous(breaks = lm0$coefficients[1], labels = "beta_1") +
  theme(axis.text.x = element_text(size = 50),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```
:::

::: {.column width="50%"}
```{r}
#| label: simulation-hist2
#| echo: false
#| fig-dpi: 300
#| cache: true
#| fig.height: 5.75

# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_histogram(data = line_df, aes(slope), fill = hp, alpha = 0.75, bins = 40) +
  geom_vline(xintercept = lm0$coefficients[2], size = 2, color = "darkslategray") +
  scale_x_continuous(breaks = lm0$coefficients[2], labels = "beta_2") +
  theme(axis.text.x = element_text(size = 50),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```
:::

::::

Simulating 1,000 draws

---

Plotting the distributions of the [point estimates]{.note} in a histogram

<br>
<br>

```{r}
#| label: gen-dataset-big
#| include: false
#| cache: true

# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 4, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

:::: {.columns}

::: {.column width="50%"}
```{r}
#| label: simulation-hist1-big
#| echo: false
#| fig-dpi: 300
#| cache: true
#| fig.height: 5.75

# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_histogram(data = line_df, aes(intercept), fill = hp, alpha = 0.75, bins = 40) +
  geom_vline(xintercept = lm0$coefficients[1], size = 2, color = hi) +
  scale_x_continuous(breaks = lm0$coefficients[1], labels = "beta_1") +
  theme(axis.text.x = element_text(size = 50),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```
:::

::: {.column width="50%"}
```{r}
#| label: simulation-hist2-big
#| echo: false
#| fig-dpi: 300
#| cache: true
#| fig.height: 5.75

# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_histogram(data = line_df, aes(slope), fill = hp, alpha = 0.75, bins = 40) +
  geom_vline(xintercept = lm0$coefficients[2], size = 2, color = "darkslategray") +
  scale_x_continuous(breaks = lm0$coefficients[2], labels = "beta_2") +
  theme(axis.text.x = element_text(size = 50),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```
:::

::::

Simulating 10,000 draws



# _Inference_ {.inverse .note}

---
name: inference
---

---

## Inference

Our current workflow:

[1.]{.note} Get data (points with $X$ and $Y$ values).

[2.]{.note} Regress $Y$ on $X$.

[3.]{.note} Plot the [point estimates]{.note} (*i.e.*, $\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1X_i$) and report.

<br>

. . .

But when do we [learn something?]{.note} [We are missing a step.]{.fragment}

. . .

- _For $\hat{\beta}_2$, can we rule out previously hypothesized values?_
- _How confident should we be in the precision of our estimates?_

. . .

We need to be careful about our sample being atypical. [AKA uncertainty.]{.fragment .note}

---

::: {.vertical-center}
However, there is a [problem]{.hi-red}.
:::

---

## {data-visibility="uncounted"}

::: {.vertical-center}
Recall the variance of the [point estimate]{.note} $\hat{\beta_1}$
$$
\mathop{\text{Var}}(\hat{\beta}_1) = \frac{{\color{#000000} \sigma^2}}{\sum_{i=1}^n (X_i - \bar{X})^2}
$$

[The problem is that ${\color{#ffffff} \sigma^2}$ is unobserved. So what do we do? _Estimate it._]{.white}

:::

---

## {data-visibility="uncounted"}

::: {.vertical-center}
Recall the variance of the [point estimate]{.note} $\hat{\beta_1}$
$$
\mathop{\text{Var}}(\hat{\beta}_1) = \frac{{\color{#BF616A} \sigma^2}}{\sum_{i=1}^n (X_i - \bar{X})^2}
$$

The problem is that ${\color{#BF616A} \sigma^2}$ is unobserved. [So what do we do?]{.note .fragment} [_Estimate it._]{.fragment .note}

:::

---

## Estimating error variance

We can estimate the variance of $u_i$ (${\color{#BF616A} \sigma^2}$) using the sum of squared residuals ([RSS]{.hi-orange}):

$$
s^2_u = \dfrac{\sum_i \hat{u}_i^2}{n - k}
$$

where $n$ is the number of observations and $k$ is the number of regression parameters. [(In a simple linear regression, $k=2$.)]{.fragment}

. . .

If the assumptions from [Gauss-Markov]{.note} hold, then $s^2_u$ is an unbiased estimator of $\sigma^2$.

. . .

In essence, we are _learning from our prediction errors_

---

## OLS Variance

With $s^2_u = \dfrac{\sum_i \hat{u}_i^2}{n - k}$, we can calculate the [estimated variance]{.note} of $\hat{\beta}_2$

$$
\mathop{\text{Var}}(\hat{\beta}_2) = \frac{s^2_u}{\sum_{i=1}^n (X_i - \bar{X})^2}
$$

. . .

Taking the square root, we get the [standard error]{.note} of the OLS estimator:

$$
\mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right) = \sqrt{ \frac{s^2_u}{\sum_{i=1}^n (X_i - \bar{X})^2} }
$$

The [standard error]{.note} is the [standard deviation]{.hi} of the [sampling distribution.]{.note}


---

## Inference

After deriving the distribution of $\hat{\beta}_2$^[*Hint:* It's normal with mean $\beta_2$ and variance $\frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}$.], we have two (related) options for formal statistical inference (learning) about our unknown parameter $\beta_2$:

<br>

- [Hypothesis testing:]{.hi} Determine whether there is statistically significant evidence to reject a hypothesized value or range of values.
- [Confidence intervals:]{.hi} Use the estimate and its standard error to create an interval that will generally^[_E.g._, similarly constructed 95% confidence intervals will contain the true parameter 95% of the time.] contain the true parameter.





# _Hypothesis testing_ {.inverse .note}

---
name: hypothesis
---

---

## Hypothesis Tests

Systematic procedure that gives us evidence to hang our hat on. Starting with a [Null hypothesis]{.hi} (H[0]{.sub}) and an [Alternative hypothesis]{.hi} (H[A]{.sub})

::: {.align-center}
H[0]{.sub}: $\beta_2 = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta_2 \neq 0$
:::

. . .

There are four possible outcomes of our test:

[1.]{.note} We [fail to reject]{.hi} the null hypothesis and the null is true.

[2.]{.note} We [reject]{.hi-red} the null hypothesis and the null is false.

[3.]{.note} We [reject]{.hi-red} the null hypothesis, but the null is actually true.^[[Type I error]{.note}]

[4.]{.note} We [fail to reject]{.hi} the null hypothesis, but the null is actually false.^[[Type II error]{.note}]

---

```{r}
#| echo: false
#| fig-align: center
#| fig-cap: "Type I vs Type II"
#| fig-height: 5.75

# Set parameters for the null and alternative hypothesis distributions
mu_null <- 0
mu_alt <- 2
sigma <- 1
alpha <- 0.05

# Find the critical value (z-score) for the significance level
critical_value <- qnorm(1 - alpha, mean = mu_null, sd = sigma)

# Generate data for the null and alternative hypothesis distributions
null_data <- data.frame(x = seq(-4, 4, 0.01), y = dnorm(seq(-4, 4, 0.01), mean = mu_null, sd = sigma))
alt_data <- data.frame(x = seq(-4, 4, 0.01), y = dnorm(seq(-4, 4, 0.01), mean = mu_alt, sd = sigma))

# Create the ggplot
ggplot() +
  geom_line(data = null_data, aes(x, y), color = hii) +
  geom_line(data = alt_data, aes(x, y), color = hired) +
  geom_vline(xintercept = mu_null, size = 0.1, color = hii) +
  geom_vline(xintercept = mu_alt, size = 0.1, color = hired) +
  geom_vline(aes(xintercept = critical_value), linetype = "dashed", color = hi) +
  geom_label(aes(x = critical_value - 0.75, y = 0.399, label = "Critical Value"),  angle = 45, size = 4) +
  xlim(-4, 4) +
  ylim(0, 0.5) +
  coord_cartesian(ylim = c(0, 0.4)) +
  xlab("X") +
  ylab("Density") +
  # ggtitle("Type I and Type II Errors") +
  scale_color_manual(values = c(hii, hired), labels = c("Null Hypothesis", "Alternative Hypothesis")) +
  labs(color = "Hypothesis") +
  geom_area(data = null_data %>% filter(x >= critical_value), aes(x, y), fill = hii, alpha = 0.5) +
  geom_area(data = alt_data %>% filter(x <= critical_value), aes(x, y), fill = hired, alpha = 0.5) +
  geom_label(aes(x = critical_value + 0.5, y = 0.01, label = "Type I Error"), size = 2.5) +
  geom_label(aes(x = critical_value - 0.5, y = 0.01, label = "Type II Error"), size = 2.5) +
  geom_label(aes(x = critical_value - 1.75, y = 0.2, label = "Null hypothesis"), size = 2.5) +
  geom_label(aes(x = critical_value + .75, y = 0.2, label = "Alt hypothesis"), size = 2.5) +
  mytheme_void



```

---

Or...

. . .

![How I think of it](typeI_vs_typeII-again.png){width=700}

---

## Hypothesis Tests

[Goal:]{.note} Make a statement about $\beta_2$ using information on $\hat{\beta}_2$.

. . .

$\hat{\beta}_2$ is random---it could be anything, even if $\beta_2 = 0$ is true.

- But if $\beta_2 = 0$ is true, then $\hat{\beta}_2$ is unlikely to take values far from zero.
- As the standard error shrinks, we are even less likely to observe "extreme" values of $\hat{\beta}_2$ (assuming $\beta_2 = 0$).

. . .

Hypothesis testing takes [extreme values]{.hi-red} of $\hat{\beta}_2$ as [_evidence against the null hypothesis_]{.note}, [but it will weight them by information about variance the [estimated variance]{.note} of $\hat{\beta}_2$.]{.fragment}

---

## Hypothesis Tests

::: {.align-center}
H[0]{.sub}: $\beta_2 = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta \neq 0$
:::

To conduct the test, we calculate a $t$-statistic^[$\beta_2^0$ is the value of $\beta_2$ in our null hypothesis (*e.g.,* $\beta_2^0 = 0$).]:

$$
t = \frac{\hat{\beta}_2 - \beta_2^0}{\mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)}
$$

Distributed by a $t$-distribution with $n-2$ _degrees of freedom_^[represents the number of independent values in a sample that are free to vary when estimating statistical parameters.].


---

## Hypothesis Tests

Next, we use the $\color{#434C5E}{t}$[-statistic]{.hi} to calculate a $\color{#B48EAD}{p}$[-value]{.hp}.

```{r}
#| echo: false
#| fig.height: 3.75
#| fig-align: center

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
t <- qt(c(.025,.8), 100)
tail_right <- rbind(c(t[2],0), subset(df, x > t[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = hp) +
  geom_vline(xintercept = qt(0.8, 100), size = 1, linetype = "solid", color = hi) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

Describes the probability of seeing a $\color{#434C5E}{t}$[-statistic]{.hi} as extreme as the one we observe _if the null hypothesis is actually true_.

. . .

But...we still need some benchmark to compare our $\color{#B48EAD}{p}$[-value]{.hp} against.

---

## Hypothesis Tests

We worry mostly about false positives, so we conduct hypothesis tests based on the probability of making a [Type I error]{.note}^[We _reject_ the null hypothesis, but the null is actually true.].

. . .

[_How?_]{.blue} [We select a [significance level]{.note}, $\color{#434C5E}{\alpha}$, that specifies our tolerance for false positives (i.e., the probability of [Type I error]{.note} we choose to live with).]{.fragment}

. . .

```{r}
#| echo: false
#| fig.height: 3.75
#| fig-align: center

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
crit <- qt(c(.025,.975), 100)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, 100) & x >= qt(0.975, 100)), aes(x, y), fill = hp) +
  #geom_vline(xintercept = qt(0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  #geom_vline(xintercept = qt(1 - 0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

---

## Hypothesis Tests

We then compare $\color{#434C5E}{\alpha}$ to the $\color{#B48EAD}{p}$[-value]{.hp} of our test.

- If the $\color{#B48EAD}{p}$[-value]{.hp} is less than $\color{#434C5E}{\alpha}$, then we [reject the null hypothesis]{.hi-red} at the $\color{#434C5E}{\alpha}\cdot100$ percent level.

- If the $\color{#B48EAD}{p}$[-value]{.hp} is greater than $\color{#434C5E}{\alpha}$, then we [fail to reject the null hypothesis]{.hi} at the $\color{#434C5E}{\alpha}\cdot100$ percent level.^[[Note:]{.note} _Fail to reject_ $\neq$ _accept_.]

---

## Hypothesis Tests

[Ex.]{.ex} Are campus police associated with campus crime?

```{r}
campus <- get(data(campus)) %>% 
  mutate(crime = round(crime/enroll*1000, 2),
         police = round(police/enroll*1000, 2)) %>% 
  filter(police < 10) %>% # remove outlier
  select(crime, police)
```

```{r}
#| echo: true

lm(crime ~ police, data = campus) %>% tidy()
```

<br>

. . .

:::: {.columns}

::: {.column width="30%"}
::: {.align-center}
H[0]{.sub}: $\beta_\text{Police} = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta_\text{Police} \neq 0$
:::
:::

::: {.column width="70%"}
Significance level: $\color{#434C5E}{\alpha} = 0.05$ (*i.e.,* 5 percent test)

Test Condition: Reject H[0]{.sub} if $p < \alpha$

[_What is the $\color{#B48EAD}{p}$[-value]{.hp}?_]{.fragment} [$p = 0.18$]{.fragment}

[_Do we reject the null hypothesis?_]{.fragment} [No.]{.fragment .hi}

:::

::::

---

## Hypothesis Tests

$\color{#B48EAD}{p}$[-values]{.hp} are difficult to calculate by hand.

[Alternative:]{.note} Compare $\color{#434C5E}{t}$[-statistic]{.hi} to [critical values]{.note} from the ${\color{#434C5E} t}$-distribution.

```{r}
#| echo: false
#| fig.height: 3.75

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
crit <- qt(c(.025,.975), 100)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, 100) & x >= qt(0.975, 100)), aes(x, y), fill = hii) +
  geom_vline(xintercept = qt(0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  geom_vline(xintercept = qt(1 - 0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  geom_vline(xintercept = 1, linetype = "solid", color = hp) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

---

## Hypothesis Tests

[Notation:]{.note} $t_{1-\alpha/2, n-2}$ or $t_\text{crit}$.

- Find in a $t$-table using $\color{#434C5E}{\alpha}$ and $n-2$ degrees of freedom.

Compare the the critical value to your $t$-statistic:

- If $|t| > |t_{1-\alpha/2, n-2}|$, then [reject the null]{.hi-red}.
- If $|t| < |t_{1-\alpha/2, n-2}|$, then [fail to reject the null]{.hi}.


---

## Two-sided tests

Based on a critical value of $t_{1-\alpha/2, n-2} = t_{0.975, 100} =$ `r round(qt(0.975, 100), 2)`, we can identify a [rejection region]{.hii} on the $\color{#434C5E}{t}$[-distribution]{.hi}.

```{r}
#| echo: false
#| fig.height: 3.75

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
crit <- qt(c(.025,.975), 100)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = hii) +
  geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, 100) & x >= qt(0.975, 100)), aes(x, y), fill = hii) +
  geom_vline(xintercept = qt(0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  geom_vline(xintercept = qt(1 - 0.975, 100), size = 0.35, linetype = "dashed", color = hi) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

. . .

If our $\color{#434C5E}{t}$[-statistic]{.hi} is in the rejection region, then we reject the null hypothesis at the 5 percent level. 

---

## Two-sided tests

[Ex.]{.ex} [R]{.mono} defaults to testing hypotheses against the null hypothesis of zero.

```{r}
#| label: hypothesis test
#| echo: true

lm(y ~ x, data = pop_df) %>% tidy()
```

. . .

:::: {.columns}

::: {.column width="50%"}
::: {.align-center}
<br>

H[0]{.sub}: $\beta_2 = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta_2 \neq 0$
:::
:::

::: {.column width="50%"}

Significance level: $\alpha = 0.05$

$$
\color{#434C5E}{t \text{-stat}} = 7.15
$$

$$
{\color{#434C5E}{t_{\text{0.975, 28}}}} = `r qt(0.975, 28) %>% round(2)`
$$

[Which implies that  $p < 0.05$.]{.fragment} [Therefore, ]{.fragment}

[we [reject H[0]{.sub}]{.hi-red} at the 5% level.]{.fragment}

:::

::::


---

## Two-sided tests

[Ex.]{.ex} Are campus police associated with campus crime?

```{r}
#| echo: true

lm(crime ~ police, data = campus) %>% tidy()
```


:::: {.columns}

::: {.column width="50%"}
::: {.align-center}

<br>

H[0]{.sub}: $\beta_\text{Police} = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta_\text{Police} \neq 0$
:::
:::

::: {.column width="50%"}

Significance level: $\alpha = 0.1$

$$
\color{#434C5E}{t \text{-stat}} = 1.35
$$

$$
{\color{#434C5E}{t_{\text{0.975, 28}}}} = `r qt(0.95, 94) %>% round(2)`
$$

[Which implies that  $p > 0.05$.]{.fragment} [Therefore, ]{.fragment}

[we [reject H[0]{.sub}]{.hi-red} at the 5% level.]{.fragment}

:::

::::


---

## One-sided tests

We might be confident in a parameter being [non-negative]{.hii}/[non-positive]{.hi-red}.

[One-sided]{.note} tests assume that the parameter of interest is either [greater than]{.hii}/[less than]{.hi-red} H[0]{.sub}.

- [Option 1]{.note} H[0]{.sub}: $\beta_2 = 0$ *vs.* H[a]{.sub}: $\beta_2 > 0$

- [Option 2]{.note} H[0]{.sub}: $\beta_2 = 0$ *vs.* H[a]{.sub}: $\beta_2 < 0$

. . .

If this assumption is reasonable, then our rejection region changes.

- Same $\alpha$.

---

## One-sided tests

[Left-tailed:]{.note} Based on a critical value of $t_{1-\alpha, n-2} = t_{0.95, 100} =$ `r round(qt(0.95, 100), 2)`, we can identify a [rejection region]{.hii} on the $t$-distribution. 

```{r}
#| echo: false
#| fig.height: 3.75

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
crit <- qt(c(.05,.95), 100)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = hii) +
  #geom_polygon(data = tail_right, aes(x=x, y=y), fill = hp) +
  #geom_vline(xintercept = qt(0.95, 100), size = 0.35, linetype = "dashed", color = hi) +
  geom_vline(xintercept = qt(1 - 0.95, 100), size = 0.35, linetype = "dashed", color = hi) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

. . .

If our $t$ statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level. 

---

## One-sided tests

[Right-tailed:]{.note} Based on a critical value of $t_{1-\alpha, n-2} = t_{0.95, 100} =$ `r round(qt(0.95, 100), 2)`, we can identify a [rejection region]{.hii} on the $t$-distribution. 

```{r}
#| echo: false
#| fig.height: 3.75

df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), 100)
)
crit <- qt(c(.05,.95), 100)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  #geom_polygon(data = tail_left, aes(x=x, y=y), fill = hp) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = hii) +
  geom_vline(xintercept = qt(0.95, 100), size = 0.35, linetype = "dashed", color = hi) +
  #geom_vline(xintercept = qt(1 - 0.95, 100), size = 0.35, linetype = "dashed", color = hi) +
  mytheme_s +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

. . .

If our $t$ statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level. 

---

## One-sided tests

**Example:** Do campus police deter campus crime?

```{r}
#| echo: true

lm(crime ~ police, data = campus) %>% tidy()
```

:::: {.columns}

::: {.column width="50%"}
::: {.align-center}

<br>

H[0]{.sub}: $\beta_\text{Police} = 0$
:::
::: {.align-center}
H[A]{.sub}: $\beta_\text{Police} < 0$
:::
:::

::: {.column width="50%"}

Significance level: $\alpha = 0.1$

$$
\color{#434C5E}{t \text{-stat}} = 1.35
$$

$$
{\color{#434C5E}{t_{\text{critical}}}} = `r qt(0.9, 94) %>% round(2)`
$$

[Which implies that  $p > 0.05$.]{.fragment} [Therefore, ]{.fragment}

[we [reject H[0]{.sub}]{.hi-red} at the 5% level.]{.fragment}

:::

::::





# _Confidence intervals_ {.inverse .note}

---
name: ci
---

---

## Confidence intervals

Until now, we have considered [point estimates]{.note} of population parameters.

- Sometimes a range of values is more interesting/honest.

. . .

We can construct $(1-\alpha)\cdot100$-percent level confidence intervals for $\beta_2$

$$
\hat{\beta}_2 \pm t_{1-\alpha/2, n-2} \, \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$

. . .

$t_{1-\alpha/2,n-2}$ denotes the $1-\alpha/2$ quantile of a $t$ distribution with $n-2$ degrees of freedom.

---

## Confidence intervals

[Q:]{.note} _Where does the confidence interval formula come from?_

. . .

[A:]{.note} Formula is a result from the rejection condition of a two-sided test.

[Reject]{.hi-red} H[0]{.sub} if 

$$
|t| > t_\text{crit}
$$

. . .

The test condition implies that we:

[Fail to reject]{.hii} H[0]{.sub} if

$$
|t| \leq t_\text{crit}
$$


Which is equivalent to:

[Fail to reject]{.hii} H[0]{.sub} if 
$$
-t_\text{crit} \leq t \leq t_\text{crit}
$$

---

## Confidence intervals {data-visibility="uncounted"}

Replacing $t$ with its formula gives:

[Fail to reject]{.hii} H[0]{.sub} if 

$$-t_\text{crit} \leq \frac{\hat{\beta}_2 - \beta_2^0}{\mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)} \leq t_\text{crit}
$$

. . .

Standard errors are always positive, so the inequalities do not flip when we multiply by $\mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)$:

[Fail to reject]{.hii} H[0]{.sub} if 
$$
-t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right) \leq \hat{\beta}_2 - \beta_2^0\leq t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$


---

## Confidence intervals {data-visibility="uncounted"}

Subtracting $\hat{\beta}_2$ yields

[Fail to reject]{.hii} H[0]{.sub} if 
$$
-\hat{\beta}_2 -t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right) \leq - \beta_2^0 \leq - \hat{\beta}_2 + t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$

. . .

Multiplying by -1 and rearranging gives

[Fail to reject]{.hii} H[0]{.sub} if 

$$
\hat{\beta}_2 - t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right) \leq \beta_2^0 \leq \hat{\beta}_2 + t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$

---

## Confidence intervals {data-visibility="uncounted"}

Replacing $\beta_2^0$ with $\beta_2$ and dropping the test condition yields the interval:

$$
\hat{\beta}_2 - t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right) \leq \beta_2 \leq \hat{\beta}_2 + t_\text{crit} \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$

which is equivalent to 

$$
\hat{\beta}_2 \pm t_\text{crit} \, \mathop{\hat{\text{SE}}} \left( \hat{\beta}_2 \right)
$$

---

## Confidence intervals

[Main insight:]{.note} 

- If a 95 percent confidence interval contains zero, then we [fail to reject]{.hii} the null hypothesis at the 5 percent level.
- If a 95 percent confidence interval does not contain zero, then we [reject]{.hi-red} the null hypothesis at the 5 percent level.

<br>
<br>

_Generally,_ a $(1- \alpha) \cdot 100$ percent confidence interval embeds a two-sided test at the $\alpha \cdot 100$ level.

---

## Confidence intervals [Ex.]{.ex}

```{r}
#| echo: true

lm(y ~ x, data = pop_df) %>% tidy()
```

<br>

. . .

```{r}
#| echo: true

# find degrees of freedom
dof <- summary(lm(y ~ x, data = pop_df))$df[2]
# return critical value
qt(0.975, dof)
```

<br>

. . .

[_95% confidence interval_]{.note} for $\beta_2$ is:

$$
0.567 \pm 1.98 \times 0.0793 = \left[ 0.410,\, 0.724 \right]
$$

---

## Confidence intervals

We have a confidence interval for $\beta_2$, *i.e.,* $\left[ 0.410,\, 0.724 \right]$

<br>

[_What does it mean?_]{.fragment}

<br>

. . .

[_Informally:_]{.note} The confidence interval gives us a region (interval) in which we can place some trust (confidence) for containing the parameter.

. . .

[More formally:]{.note} If we repeatedly sample from our population and construct confidence intervals for each of these samples, then $(1-\alpha) \cdot100$ percent of our intervals (*e.g.,* 95%) will contain the population parameter *somewhere in the interval*.

---

## Confidence intervals

We drew _10,000_ samples (each of size $n = 30$) from our population and estimated our regression model for each sample:

$$ Y_i = \hat{\beta}_1 + \hat{\beta}_2 X_i + \hat{u}_i $$
<center>[_(repeated 10,000 times)_]{.note .small}</center>

Now, let's estimate 95% confidence intervals for each of these intervals...

---

## Confidence intervals

```{r}
#| include: false


# Create confidence intervals for b1
ci_df <- sim_df %>% filter(term == "x") %>%
  mutate(
    lb = estimate - std.error * qt(.975, 28),
    ub = estimate + std.error * qt(.975, 28),
    ci_contains = (lm0$coefficients[2] >= lb) & (lm0$coefficients[2] <= ub),
    ci_above = lm0$coefficients[2] < lb,
    ci_below = lm0$coefficients[2] > ub,
    ci_group = 2 * ci_above + (!ci_below)
  ) %>%
  arrange(ci_group, estimate) %>%
  mutate(x = 1:1e4)
```

_From our previous simulation_, `r ci_df$ci_contains %>% multiply_by(100) %>% mean() %>% round(1)`% of 95% confidence intervals contain the true parameter value of $\beta_2$.

```{r}
#| echo: false
#| fig.width: 10

# Plot
ggplot(data = ci_df) +
geom_segment(aes(y = lb, yend = ub, x = x, xend = x, color = ci_contains)) +
geom_hline(yintercept = lm0$coefficients[2]) +
scale_y_continuous(breaks = lm0$coefficients[2]) +
scale_color_manual(values = c(hp, "grey85")) +
mytheme_s +
theme(
  axis.text.x = element_blank(),
  legend.position = 'none',
  axis.text.y = element_text(size = 18)) + xlab("") + 
  ylab("") 
```

---

## [Ex.]{.ex} Association of police with crime

You can instruct `tidy` to return a 95 percent confidence interval for the association of campus police with campus crime:

```{r}
#| echo: true

lm(crime ~ police, data = campus) %>% 
  tidy(conf.int = TRUE, conf.level = 0.95)
```

---

## [Ex.]{.ex} Association of police with crime

```{r}
#| fig.height: 5
#| echo: false

reg <- lm(crime ~ police, data = campus)

conf1 <- tidy(reg, conf.int = TRUE, conf.level = 0.75) %>% 
  filter(term == "police") %>% 
  mutate(term = "75% CI")
conf2 <- tidy(reg, conf.int = TRUE, conf.level = 0.9) %>% 
  filter(term == "police") %>% 
  mutate(term = "90% CI")
conf3 <- tidy(reg, conf.int = TRUE, conf.level = 0.95) %>% 
  filter(term == "police") %>% 
  mutate(term = "95% CI")
conf4 <- tidy(reg, conf.int = TRUE, conf.level = 0.99) %>% 
  filter(term == "police") %>% 
  mutate(term = "99% CI")

conf <- bind_rows(conf1, conf2, conf3, conf4)

conf %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange(color = hp, size = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", size = 1) +
  coord_flip() +
  mytheme_s +
  theme(
    axis.title.y = element_text(size = 18, angle = 90),
    axis.title.x = element_text(size = 18)
  ) +
  ylab("Value") +
  xlab("Police Coefficient")
```

Four confidence intervals for the same coefficient.



<!-- TODO: This lecture kind of sucked. I think a major revamp is necessary. -->


```{r}
#| include: false
#| echo: false
#| eval: false
library(renderthis)

renderthis::to_pdf(
  from = "./slides/005-inference/050-main.html",
  to = "./slides/005-inference/050-main.pdf",
  complex_slides = TRUE
)
```
